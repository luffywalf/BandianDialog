前面的log不小心删掉了
现在状况：【pos做三份一起训练】0.97> 【一份pos多训练几次】0.69 第二种情况可能是我不会调参 或者cnn过采样平衡的原因？
【pos做三份一起训练】:[0.9763636363636364, 0.9743589743589743, 0.9780701754385965, 0.9738714618714618]
##batch=40 lr= 1e-4 epoch=15 f1=0.19
#batch=40 lr= 0.01 epoch=20 f1=0.69
#batch=40 lr= 0.01 epoch=10 f1=0.55
#batch=40 lr= 0.01 epoch=5 f1=0.61
#batch=40 lr= 0.01 epoch=15 f1=0.64
又试了几套参数先过

#先改数据试试看
改bandian_pos_copy
四类机构里面（各50）我会将约一半的改短 【我属于中国人寿保险股份有限公司】-> 【我属于中国人寿保险】
第20类：企业或工商客户  一半该短 x-194+1=25 x=25+193=218
第22类：机关事业单位 好像不太好改短 尽力改把 x=25+264=298 基本没变动 注意中国人民银行属于机关 其他银行是企业
第39类：企业 469+25=494
第40类没改 本来就是长短不一的

[0.9701298701298701, 0.9607226107226108, 0.9692982456140351, 0.9617946497946498]
我是中兴的	39	22*
我是中兴公司的	39	22*
我是兴业银行的	39	22*
我属于兴业银行	39	39 &

说明抓重点的能力还是不行，直接想法 1.attention获取重点但是我怕过拟合之类的 2.boosting的算法有加重分错数据比重的能力 但是现在是CNN
3.我知道现在fasttext的文本分类普遍效果比较好 且对数据不平衡问题解决比较好 也比较适合数据量少 因为网络结构简单 但对于抓重点这个不知道效果如何，感觉不会太好


光想是不行了 我们来试试把 反正fasttext也好试
3*pos - pre=0.7894736842105263
1*pos - pre=0.11842105263157894
2*pos - pre=0.618
4*pos - pre=0.9372937293729373
5*pos - pre=0.9551451187335093
6*pos - pre=0.967032967032967
这个虽然写起来简单 但是封装太多了 这怎么调参怎么改呀～


按xk所说 train*3/dev*3/test*3 训练了一次  和train/dev/test是一样的       pos=1份
[0.6310966810966812, 0.644973544973545, 0.618421052631579, 0.6254062736205593]
所以 这个和数据分布是有一定关系的 就是在[pos*3]训练时 是对数据分布不懂 反正是有益的
这个问题先过～

下面的问题 就主要还是 四类的错误：
要在fasttext上改 还是cnn上加attention

https://www.zhihu.com/question/60688149
想要在文本分类问题上加词向量，首先你要先加上词向量

词向量
"语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型"
所以 我现在的语料too small to train a w2v


我是代表中国建设银行来的	39	39
我属于中国建设银行	39	39
我是中国建设银行的	39	39 原句
我属于建设银行	39	40
我是代表建设银行来的	39	40
我是建设银行的	39	40

# 如上 表现觉得真的很不好 就拿前四个来举例

发现是unk的问题 加上词向量
[0.9827452421792044, 0.9817610062893082, 0.9780701754385965, 0.980719978553269]
四类的分类问题又准确率一些

